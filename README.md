# Awesome-LLM-Fine-Tuning-Alignment
LLM fine-tuning 



# Papers 


## Unsupervised Fine-Tuning 

### Unsupervised Full Fine-Tuning

### Contrastive Learning 


## Supervised Fine-Tuning Methods

### Parameter-Efficient Fine-Tuning (PEFT)


### Supervised Full Fine-Tuning 


### Instruction Fine-Tuning 

**Finetuned Language Models are Zero-Shot Learners** \
*Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V Le* \
ICLR 2022. [[Paper](https://openreview.net/forum?id=gEZrGCozdqR)]

**Instruction Tuning for Large Language Models: A Survey** \
*Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang* \
arXiv 2024. [[Paper](https://arxiv.org/abs/2308.10792)] [[Github](https://github.com/xiaoya-li/instruction-tuning-survey)] 


### RLHF & DPO

**Direct Preference Optimization: Your Language Model is Secretly a Reward Model** \
*Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn*  \
NeurIPS 2023. [[Paper](https://openreview.net/forum?id=HPuSIXJaa9&utm_source=substack&utm_medium=email)]


## RAG vs Fine-tuning 

**RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture** \
*Angels Balaguer, et al* \
arXiv 2024. [[Paper](https://arxiv.org/abs/2401.08406)]

**Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge**  \
*Heydar Soudani, Evangelos Kanoulas, Faegheh Hasibi* \
arXiv 2024. [[Paper](https://arxiv.org/abs/2403.01432)][[Github](https://github.com/heydarsoudani/ragvsft)]

**Reliable, Adaptable, and Attributable Language Models with Retrieval** \
*Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, Wen-tau Yih* \
arXiv 2024. [[Paper](https://arxiv.org/abs/2403.03187)]

**Long-form factuality in large language models** \
*Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le* \
arXiv 2024. [[Paper](https://arxiv.org/abs/2403.18802)][[Github](https://github.com/google-deepmind/long-form-factuality)]


## Data quality, knowledge injection, TBD


**QuRating: Selecting High-Quality Data for Training Language Models**  \
*Alexander Wettig, Aatmik Gupta, Saumya Malik, Danqi Chen* \
arXiv 2024. [[Paper](https://arxiv.org/abs/2402.09739)][[Github](https://github.com/princeton-nlp/qurating)]

**LESS: Selecting Influential Data for Targeted Instruction Tuning** \
*Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, Danqi Chen* \
arXiv 2024. [[Paper](https://arxiv.org/abs/2402.04333)][[Github](https://github.com/princeton-nlp/less)]

**Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs** \
*Oded Ovadia, Menachem Brief, Moshik Mishaeli, Oren Elisha* \
arXiv 2023. [[Paper](https://arxiv.org/abs/2312.05934)]

**Plug-and-Play Knowledge Injection for Pre-trained Language Models** \
*Zhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Huadong Wang, Deming Ye, Chaojun Xiao, Xu Han, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou* \
ACL 2023. [[Paper](https://arxiv.org/abs/2305.17691)][[Github](https://github.com/thunlp/knowledge-plugin)]



## Resources 

*Awesome LLMs Fine-Tuning* \
https://github.com/Curated-Awesome-Lists/awesome-llms-fine-tuning 

*How to Fine-Tune LLMs in 2024 with Hugging Face* \
https://www.philschmid.de/fine-tune-llms-in-2024-with-trl 

*Huggingface Blog* \
https://huggingface.co/blog

*Instruction Tuning: What is fine-tuning?* \
https://datascientest.com/en/instruction-tuning-what-is-fine-tuning#:~:text=Whereas%20supervised%20fine%2Dtuning%20consists,more%20easily%20to%20new%20tasks.

*SOTA Knowledge Injection?* 
https://www.reddit.com/r/LocalLLaMA/comments/1as5bn4/sota_knowledge_injection/
https://www.reddit.com/r/LocalLLaMA/comments/1ao2bzu/best_way_to_add_knowledge_to_a_llm/
https://www.youtube.com/watch?v=xz-HRZhjkWk

<!-- course
https://github.com/aishwaryanr/awesome-generative-ai-guide/blob/main/free_courses/Applied_LLMs_Mastery_2024/week3_finetuning_llms.md
 -->
