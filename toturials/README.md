# Awesome-LLM-Fine-Tuning-Alignment
LLM fine-tuning 



# Papers 


## Pretraining 

### Continual Pre-training 




### Continual learning
**Continual Learning for Large Language Models: A Survey**  \
*Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, Gholamreza Haffari* \
arXiv 2024. [[Paper](https://arxiv.org/abs/2402.01364)]



## Unsupervised Fine-Tuning 

### Unsupervised Full Fine-Tuning

### Contrastive Learning 


## Supervised Fine-Tuning Methods

**LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models** \
*Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Yongqiang Ma* \
ArXiv 2024. [[Paper](https://arxiv.org/abs/2403.13372)][[Github](https://github.com/hiyouga/llama-factory)] 

### Parameter-Efficient Fine-Tuning (PEFT)


### Supervised Full Fine-Tuning 


### Instruction Fine-Tuning 

**Finetuned Language Models are Zero-Shot Learners** \
*Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V Le* \
ICLR 2022. [[Paper](https://openreview.net/forum?id=gEZrGCozdqR)]

**Instruction Tuning for Large Language Models: A Survey** \
*Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang* \
arXiv 2024. [[Paper](https://arxiv.org/abs/2308.10792)] [[Github](https://github.com/xiaoya-li/instruction-tuning-survey)] 


### RLHF & DPO

**Learn Your Reference Model for Real Good Alignment** \
*Alexey Gorbatovski, Boris Shaposhnikov, Alexey Malakhov, Nikita Surnachev, Yaroslav Aksenov, Ian Maksimov, Nikita Balagansky, Daniil Gavrilov* \
arXiv 2024. [[Paper](https://arxiv.org/abs/2404.09656)]

sw
**Direct Preference Optimization: Your Language Model is Secretly a Reward Model** \
*Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, Chelsea Finn*  \
NeurIPS 2023. [[Paper](https://openreview.net/forum?id=HPuSIXJaa9&utm_source=substack&utm_medium=email)]



